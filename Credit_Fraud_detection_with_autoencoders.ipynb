{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Credit Fraud detection with autoencoders.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpanagop/ML_and_AI_examples/blob/master/Credit_Fraud_detection_with_autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEl3ncgPc0sv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#Credit Card Fraud Detection\n",
        "\n",
        "Example of outlier detection with autoencoders. Dataset https://www.kaggle.com/mlg-ulb/creditcardfraud from Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles).\n",
        "\n",
        "It is a highly unbalanced dataset with a very low percetnage of fraudulent credit card transactions. Our purpose is to build a classifier for detecting fraudulent transactions. In this example we will consider them as outliers an will use an autoencoder for detecting them.\n",
        "\n",
        "##Downloading of dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT2psZNVvB9U",
        "colab_type": "code",
        "outputId": "2a781f3d-7b0e-4940-e228-0e87063f5f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "!wget -O creditfraud.zip https://www.dropbox.com/s/tl20yp9bcl56oxt/creditcardfraud.zip?dl=0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-28 04:07:01--  https://www.dropbox.com/s/tl20yp9bcl56oxt/creditcardfraud.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:6016:1::a27d:101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/tl20yp9bcl56oxt/creditcardfraud.zip [following]\n",
            "--2019-11-28 04:07:01--  https://www.dropbox.com/s/raw/tl20yp9bcl56oxt/creditcardfraud.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc00eb81e39e15f3e7eb2a63114c.dl.dropboxusercontent.com/cd/0/inline/AtO36T9HrRh1lv1X4fjHDp-az5GX_fQBx6A61o9S1_nNE-TcE7NM7JZn0DGhfBBlU8mOYfEJHXW3CQtlDePz3MNOWL2idFgKSLncClCeI9wGThnDQolRYxt3iLzCqPRbRHw/file# [following]\n",
            "--2019-11-28 04:07:01--  https://uc00eb81e39e15f3e7eb2a63114c.dl.dropboxusercontent.com/cd/0/inline/AtO36T9HrRh1lv1X4fjHDp-az5GX_fQBx6A61o9S1_nNE-TcE7NM7JZn0DGhfBBlU8mOYfEJHXW3CQtlDePz3MNOWL2idFgKSLncClCeI9wGThnDQolRYxt3iLzCqPRbRHw/file\n",
            "Resolving uc00eb81e39e15f3e7eb2a63114c.dl.dropboxusercontent.com (uc00eb81e39e15f3e7eb2a63114c.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n",
            "Connecting to uc00eb81e39e15f3e7eb2a63114c.dl.dropboxusercontent.com (uc00eb81e39e15f3e7eb2a63114c.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/AtN69OIj7nxIMRKGYu4bXMK0az97qwRNlf1BdtnoUMdfvBOwe4NlE4X8bAGjdqxpUEeNZPPD-zrdmrdWRqixpQ4uAwzDKrlTEJMvonwTfMSAYkRCLxCVpsvLJ4mySQcoB6lcgVp_7mq1CB6AHp4V0AW00haAaRi0RIsT87o1s2qUkD6rFz4l9dsf2klpkNASCmWQ4jUQEnACWWupFY3fGlkKrcp3SJFYZV2pO_EvDN6fS9tZxstcPJNzp7wVJg5WJw8f-oQAcfCEMvnT2Lh4Xv_p69nGaqpBexhtxjUupAiQI9AiqrVZ_TxoVbiinkC3xgyqSozlue3zWOKaj0rdApsD4pe_HGQxEoLmgg8a2ctvUQ/file [following]\n",
            "--2019-11-28 04:07:02--  https://uc00eb81e39e15f3e7eb2a63114c.dl.dropboxusercontent.com/cd/0/inline2/AtN69OIj7nxIMRKGYu4bXMK0az97qwRNlf1BdtnoUMdfvBOwe4NlE4X8bAGjdqxpUEeNZPPD-zrdmrdWRqixpQ4uAwzDKrlTEJMvonwTfMSAYkRCLxCVpsvLJ4mySQcoB6lcgVp_7mq1CB6AHp4V0AW00haAaRi0RIsT87o1s2qUkD6rFz4l9dsf2klpkNASCmWQ4jUQEnACWWupFY3fGlkKrcp3SJFYZV2pO_EvDN6fS9tZxstcPJNzp7wVJg5WJw8f-oQAcfCEMvnT2Lh4Xv_p69nGaqpBexhtxjUupAiQI9AiqrVZ_TxoVbiinkC3xgyqSozlue3zWOKaj0rdApsD4pe_HGQxEoLmgg8a2ctvUQ/file\n",
            "Reusing existing connection to uc00eb81e39e15f3e7eb2a63114c.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 69155672 (66M) [application/zip]\n",
            "Saving to: ‘creditfraud.zip’\n",
            "\n",
            "creditfraud.zip     100%[===================>]  65.95M  45.5MB/s    in 1.4s    \n",
            "\n",
            "2019-11-28 04:07:04 (45.5 MB/s) - ‘creditfraud.zip’ saved [69155672/69155672]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xqM1qxSvD_b",
        "colab_type": "code",
        "outputId": "fd7ee9ea-45fe-4541-f214-ef496b71bc9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "!unzip creditfraud.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  creditfraud.zip\n",
            "replace creditcard.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: creditcard.csv          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OV8v985dDi6",
        "colab_type": "text"
      },
      "source": [
        "##Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-vN7SvXvMs5",
        "colab_type": "code",
        "outputId": "d0c3b824-37dc-41ee-f53f-33d3aed20442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp9PNETFd2G4",
        "colab_type": "text"
      },
      "source": [
        "##Loading dataset in Python and taking a first look"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMVr2-hzvlID",
        "colab_type": "code",
        "outputId": "c5d8f7c4-825b-40fd-cbb7-edc9a05804c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "dat=pd.read_csv('creditcard.csv')\n",
        "dat.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
              "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
              "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y0a-xNMeDEA",
        "colab_type": "text"
      },
      "source": [
        "The dataset is highly unbalanced with very few fraudulent credit cards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3TwIbGCvmz9",
        "colab_type": "code",
        "outputId": "1846477b-f926-4228-abe7-50146d09f04d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "dat['Class'].value_counts()/dat['Class'].count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.998273\n",
              "1    0.001727\n",
              "Name: Class, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4KRErdPvt1A",
        "colab_type": "code",
        "outputId": "95e08e75-11e6-47f8-c377-4d3e7895728d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "sns.countplot(x='Class',data=dat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f343caa49e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEGCAYAAACpXNjrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDJt4sXDvxzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dat = dat.drop([ 'Time'], 1)\n",
        "dat['Amount'] = StandardScaler().fit_transform(dat['Amount'].values.reshape(-1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vTh-X3beLk_",
        "colab_type": "text"
      },
      "source": [
        "Splitting into train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T3X-3wFv001",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dat.drop('Class',1) , dat['Class'], test_size=0.5, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooXM9UEwv3QX",
        "colab_type": "code",
        "outputId": "0de04af5-f7fc-4d25-a8ed-1b349703a8ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "y_test.value_counts()/y_test.count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.998294\n",
              "1    0.001706\n",
              "Name: Class, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3YUftDjv5z-",
        "colab_type": "code",
        "outputId": "247b9428-51e2-4c19-b3ac-398955f36455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "y_train.value_counts()/y_train.count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.998251\n",
              "1    0.001749\n",
              "Name: Class, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otvOl-HcevoG",
        "colab_type": "text"
      },
      "source": [
        "##First method: using autoencoder's regression error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPpRAl0TeQR8",
        "colab_type": "text"
      },
      "source": [
        "For our first example we will train our autoencoder only on non fraudulent cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMzBdaAdv7rE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_normal = X_train[y_train==0]\n",
        "X_train_fraud = X_train[y_train==1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbX1U5aheSla",
        "colab_type": "text"
      },
      "source": [
        "Building an autoencoder with\n",
        "- an input layer with 29 neurons,\n",
        "- a hidden layer with 12 neurons,\n",
        "- an output layer with 29 neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euBPgiamw2R6",
        "colab_type": "code",
        "outputId": "afd64cdc-dcf4-4595-9496-cc3b0d89fa0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "input_layer = Input(shape=(29, ))\n",
        "encoded = Dense(12,activation='tanh')(input_layer)\n",
        "decoded = Dense(29,activation='sigmoid')(encoded)\n",
        "autoencoder = Model(input_layer,decoded)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY42GWDWyoxL",
        "colab_type": "code",
        "outputId": "ba09f0c7-e251-47c0-a269-f4af57513e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "autoencoder.compile(optimizer='adam',loss='mean_squared_error')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC2j3eXJxM-B",
        "colab_type": "code",
        "outputId": "87fb0776-1173-4a98-cd16-dd8da40fae7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "autoencoder.fit(X_train_normal, X_train_normal, epochs = 100, batch_size=128,\n",
        "validation_data=(X_train_normal,X_train_normal))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 142154 samples, validate on 142154 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "142154/142154 [==============================] - 3s 18us/step - loss: 0.9911 - val_loss: 0.8843\n",
            "Epoch 2/100\n",
            "142154/142154 [==============================] - 2s 16us/step - loss: 0.8598 - val_loss: 0.8426\n",
            "Epoch 3/100\n",
            "142154/142154 [==============================] - 2s 16us/step - loss: 0.8328 - val_loss: 0.8244\n",
            "Epoch 4/100\n",
            "142154/142154 [==============================] - 2s 16us/step - loss: 0.8182 - val_loss: 0.8125\n",
            "Epoch 5/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.8080 - val_loss: 0.8037\n",
            "Epoch 6/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.8004 - val_loss: 0.7972\n",
            "Epoch 7/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7946 - val_loss: 0.7921\n",
            "Epoch 8/100\n",
            "142154/142154 [==============================] - 3s 18us/step - loss: 0.7903 - val_loss: 0.7885\n",
            "Epoch 9/100\n",
            "142154/142154 [==============================] - 3s 19us/step - loss: 0.7870 - val_loss: 0.7855\n",
            "Epoch 10/100\n",
            "142154/142154 [==============================] - 3s 18us/step - loss: 0.7844 - val_loss: 0.7834\n",
            "Epoch 11/100\n",
            "142154/142154 [==============================] - 3s 20us/step - loss: 0.7824 - val_loss: 0.7815\n",
            "Epoch 12/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7809 - val_loss: 0.7801\n",
            "Epoch 13/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7796 - val_loss: 0.7790\n",
            "Epoch 14/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7786 - val_loss: 0.7781\n",
            "Epoch 15/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7776 - val_loss: 0.7773\n",
            "Epoch 16/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7769 - val_loss: 0.7765\n",
            "Epoch 17/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7762 - val_loss: 0.7759\n",
            "Epoch 18/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7756 - val_loss: 0.7755\n",
            "Epoch 19/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7750 - val_loss: 0.7748\n",
            "Epoch 20/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7745 - val_loss: 0.7742\n",
            "Epoch 21/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7741 - val_loss: 0.7738\n",
            "Epoch 22/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7736 - val_loss: 0.7732\n",
            "Epoch 23/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7733 - val_loss: 0.7728\n",
            "Epoch 24/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7729 - val_loss: 0.7727\n",
            "Epoch 25/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7725 - val_loss: 0.7724\n",
            "Epoch 26/100\n",
            "142154/142154 [==============================] - 2s 16us/step - loss: 0.7723 - val_loss: 0.7720\n",
            "Epoch 27/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7721 - val_loss: 0.7719\n",
            "Epoch 28/100\n",
            "142154/142154 [==============================] - 2s 16us/step - loss: 0.7719 - val_loss: 0.7718\n",
            "Epoch 29/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7716 - val_loss: 0.7715\n",
            "Epoch 30/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7715 - val_loss: 0.7715\n",
            "Epoch 31/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7712 - val_loss: 0.7711\n",
            "Epoch 32/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7711 - val_loss: 0.7713\n",
            "Epoch 33/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7710 - val_loss: 0.7707\n",
            "Epoch 34/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7708 - val_loss: 0.7708\n",
            "Epoch 35/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7706 - val_loss: 0.7706\n",
            "Epoch 36/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7705 - val_loss: 0.7705\n",
            "Epoch 37/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7704 - val_loss: 0.7704\n",
            "Epoch 38/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7703 - val_loss: 0.7705\n",
            "Epoch 39/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7703 - val_loss: 0.7701\n",
            "Epoch 40/100\n",
            "142154/142154 [==============================] - 3s 18us/step - loss: 0.7701 - val_loss: 0.7702\n",
            "Epoch 41/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7701 - val_loss: 0.7698\n",
            "Epoch 42/100\n",
            "142154/142154 [==============================] - 3s 18us/step - loss: 0.7699 - val_loss: 0.7698\n",
            "Epoch 43/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7699 - val_loss: 0.7698\n",
            "Epoch 44/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7699 - val_loss: 0.7700\n",
            "Epoch 45/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7698 - val_loss: 0.7695\n",
            "Epoch 46/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7696 - val_loss: 0.7699\n",
            "Epoch 47/100\n",
            "142154/142154 [==============================] - 3s 18us/step - loss: 0.7696 - val_loss: 0.7693\n",
            "Epoch 48/100\n",
            "142154/142154 [==============================] - 3s 18us/step - loss: 0.7695 - val_loss: 0.7694\n",
            "Epoch 49/100\n",
            "142154/142154 [==============================] - 3s 18us/step - loss: 0.7694 - val_loss: 0.7698\n",
            "Epoch 50/100\n",
            "142154/142154 [==============================] - 3s 18us/step - loss: 0.7694 - val_loss: 0.7691\n",
            "Epoch 51/100\n",
            "142154/142154 [==============================] - 3s 18us/step - loss: 0.7693 - val_loss: 0.7694\n",
            "Epoch 52/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7693 - val_loss: 0.7693\n",
            "Epoch 53/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7692 - val_loss: 0.7692\n",
            "Epoch 54/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7691 - val_loss: 0.7691\n",
            "Epoch 55/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7692 - val_loss: 0.7689\n",
            "Epoch 56/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7691 - val_loss: 0.7689\n",
            "Epoch 57/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7690 - val_loss: 0.7691\n",
            "Epoch 58/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7689 - val_loss: 0.7690\n",
            "Epoch 59/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7689 - val_loss: 0.7688\n",
            "Epoch 60/100\n",
            "142154/142154 [==============================] - 2s 16us/step - loss: 0.7689 - val_loss: 0.7687\n",
            "Epoch 61/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7688 - val_loss: 0.7687\n",
            "Epoch 62/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7688 - val_loss: 0.7686\n",
            "Epoch 63/100\n",
            "142154/142154 [==============================] - 2s 16us/step - loss: 0.7688 - val_loss: 0.7686\n",
            "Epoch 64/100\n",
            "142154/142154 [==============================] - 3s 18us/step - loss: 0.7687 - val_loss: 0.7685\n",
            "Epoch 65/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7687 - val_loss: 0.7685\n",
            "Epoch 66/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7687 - val_loss: 0.7689\n",
            "Epoch 67/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7687 - val_loss: 0.7684\n",
            "Epoch 68/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7686 - val_loss: 0.7685\n",
            "Epoch 69/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7686 - val_loss: 0.7683\n",
            "Epoch 70/100\n",
            "142154/142154 [==============================] - 2s 16us/step - loss: 0.7685 - val_loss: 0.7687\n",
            "Epoch 71/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7685 - val_loss: 0.7683\n",
            "Epoch 72/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7685 - val_loss: 0.7690\n",
            "Epoch 73/100\n",
            "142154/142154 [==============================] - 2s 16us/step - loss: 0.7685 - val_loss: 0.7685\n",
            "Epoch 74/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7684 - val_loss: 0.7688\n",
            "Epoch 75/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7685 - val_loss: 0.7684\n",
            "Epoch 76/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7683 - val_loss: 0.7682\n",
            "Epoch 77/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7683 - val_loss: 0.7684\n",
            "Epoch 78/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7683 - val_loss: 0.7681\n",
            "Epoch 79/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7683 - val_loss: 0.7682\n",
            "Epoch 80/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7683 - val_loss: 0.7684\n",
            "Epoch 81/100\n",
            "142154/142154 [==============================] - 2s 16us/step - loss: 0.7682 - val_loss: 0.7681\n",
            "Epoch 82/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7682 - val_loss: 0.7680\n",
            "Epoch 83/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7681 - val_loss: 0.7680\n",
            "Epoch 84/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7682 - val_loss: 0.7682\n",
            "Epoch 85/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7682 - val_loss: 0.7681\n",
            "Epoch 86/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7681 - val_loss: 0.7680\n",
            "Epoch 87/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7681 - val_loss: 0.7679\n",
            "Epoch 88/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7680 - val_loss: 0.7692\n",
            "Epoch 89/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7681 - val_loss: 0.7679\n",
            "Epoch 90/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7679 - val_loss: 0.7682\n",
            "Epoch 91/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7680 - val_loss: 0.7679\n",
            "Epoch 92/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7680 - val_loss: 0.7679\n",
            "Epoch 93/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7679 - val_loss: 0.7677\n",
            "Epoch 94/100\n",
            "142154/142154 [==============================] - 2s 16us/step - loss: 0.7680 - val_loss: 0.7677\n",
            "Epoch 95/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7679 - val_loss: 0.7679\n",
            "Epoch 96/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7679 - val_loss: 0.7678\n",
            "Epoch 97/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7679 - val_loss: 0.7677\n",
            "Epoch 98/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7679 - val_loss: 0.7678\n",
            "Epoch 99/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7678 - val_loss: 0.7677\n",
            "Epoch 100/100\n",
            "142154/142154 [==============================] - 2s 17us/step - loss: 0.7679 - val_loss: 0.7675\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f343aae3390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzcu7y3nxsw3",
        "colab_type": "code",
        "outputId": "d1f73fe7-8c35-402d-9123-c63bf2d2a8f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "predictions = autoencoder.predict(X_train)\n",
        "mse = np.mean(np.power(X_train - predictions, 2), axis=1)\n",
        "error_df = pd.DataFrame({'reconstruction_error': mse,\n",
        "                        'true_class': y_train})\n",
        "error_df.groupby('true_class').describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"8\" halign=\"left\">reconstruction_error</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>true_class</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142154.0</td>\n",
              "      <td>0.767519</td>\n",
              "      <td>3.439808</td>\n",
              "      <td>0.037731</td>\n",
              "      <td>0.227251</td>\n",
              "      <td>0.396262</td>\n",
              "      <td>0.645605</td>\n",
              "      <td>318.941692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>249.0</td>\n",
              "      <td>29.855354</td>\n",
              "      <td>43.107802</td>\n",
              "      <td>0.118304</td>\n",
              "      <td>4.228176</td>\n",
              "      <td>10.783928</td>\n",
              "      <td>26.808312</td>\n",
              "      <td>282.265950</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           reconstruction_error             ...                       \n",
              "                          count       mean  ...        75%         max\n",
              "true_class                                  ...                       \n",
              "0                      142154.0   0.767519  ...   0.645605  318.941692\n",
              "1                         249.0  29.855354  ...  26.808312  282.265950\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRQepwFre9UM",
        "colab_type": "text"
      },
      "source": [
        "As we can see above the error for non fraudulent case is lower than the error for fraudulent cases. We use a threshold of mean plus 3 sds to classify the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zAII8DQxw-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_predictions=autoencoder.predict(X_test)\n",
        "mse = np.mean(np.power(X_test - test_predictions, 2), axis=1)\n",
        "y_pred=[(lambda er: 1 if er>=11.078922  else 0)(er) for er in mse]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ4V1lfb46jQ",
        "colab_type": "code",
        "outputId": "ddd09775-e18f-411a-bf83-4204e1a7bfef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "conf_matrix = metrics.confusion_matrix(y_test,y_pred)\n",
        "\n",
        "ax=plt.subplot()\n",
        "sns.heatmap(conf_matrix,annot=True,ax=ax,fmt='g')#annot=True to annotate cells, fmt='g' numbers not scientific form\n",
        "ax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(['Normal', 'Fraud']); ax.yaxis.set_ticklabels(['Normal', 'Fraud']);\n",
        "ax.set(yticks=[0, 2], \n",
        "       xticks=[0.5, 1.5])\n",
        "ax.yaxis.set_major_locator(ticker.IndexLocator(base=1, offset=0.5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEWCAYAAACAOivfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh2xw3tcfng9",
        "colab_type": "text"
      },
      "source": [
        "##Second method: using encoder part of autoencoder and k-NN\n",
        "\n",
        "We train using all cases (fraud/non-fraud)in train dataset and use the result to map the instances into a 12-dimensional space. The mapped cases are fed to k-NN for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7SnYf1KFVm9",
        "colab_type": "code",
        "outputId": "dbeb5d25-878e-4684-d802-734557fff4b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "input_layer_all = Input(shape=(29, ))\n",
        "encoded_all = Dense(12,activation='tanh')(input_layer_all)\n",
        "decoded_all = Dense(29,activation='sigmoid')(encoded_all)\n",
        "autoencoder_all = Model(input_layer_all,decoded_all)\n",
        "autoencoder_all.compile(optimizer='adam',loss='mean_squared_error')\n",
        "autoencoder_all.fit(X_train, X_train, epochs = 100, batch_size=128,\n",
        "validation_data=(X_train,X_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 142403 samples, validate on 142403 samples\n",
            "Epoch 1/100\n",
            "142403/142403 [==============================] - 3s 20us/step - loss: 1.0494 - val_loss: 0.9373\n",
            "Epoch 2/100\n",
            "142403/142403 [==============================] - 3s 19us/step - loss: 0.9123 - val_loss: 0.8942\n",
            "Epoch 3/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8835 - val_loss: 0.8743\n",
            "Epoch 4/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8679 - val_loss: 0.8622\n",
            "Epoch 5/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8580 - val_loss: 0.8540\n",
            "Epoch 6/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8509 - val_loss: 0.8480\n",
            "Epoch 7/100\n",
            "142403/142403 [==============================] - 3s 18us/step - loss: 0.8457 - val_loss: 0.8436\n",
            "Epoch 8/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8419 - val_loss: 0.8402\n",
            "Epoch 9/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8389 - val_loss: 0.8376\n",
            "Epoch 10/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8365 - val_loss: 0.8353\n",
            "Epoch 11/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8344 - val_loss: 0.8336\n",
            "Epoch 12/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8328 - val_loss: 0.8321\n",
            "Epoch 13/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8315 - val_loss: 0.8308\n",
            "Epoch 14/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8303 - val_loss: 0.8297\n",
            "Epoch 15/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8294 - val_loss: 0.8290\n",
            "Epoch 16/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8284 - val_loss: 0.8280\n",
            "Epoch 17/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8277 - val_loss: 0.8272\n",
            "Epoch 18/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8270 - val_loss: 0.8266\n",
            "Epoch 19/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8263 - val_loss: 0.8260\n",
            "Epoch 20/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8258 - val_loss: 0.8257\n",
            "Epoch 21/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8255 - val_loss: 0.8251\n",
            "Epoch 22/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8250 - val_loss: 0.8247\n",
            "Epoch 23/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8246 - val_loss: 0.8243\n",
            "Epoch 24/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8243 - val_loss: 0.8244\n",
            "Epoch 25/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8241 - val_loss: 0.8237\n",
            "Epoch 26/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8238 - val_loss: 0.8239\n",
            "Epoch 27/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8236 - val_loss: 0.8234\n",
            "Epoch 28/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8233 - val_loss: 0.8231\n",
            "Epoch 29/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8231 - val_loss: 0.8228\n",
            "Epoch 30/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8228 - val_loss: 0.8226\n",
            "Epoch 31/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8227 - val_loss: 0.8225\n",
            "Epoch 32/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8225 - val_loss: 0.8222\n",
            "Epoch 33/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8223 - val_loss: 0.8224\n",
            "Epoch 34/100\n",
            "142403/142403 [==============================] - 3s 18us/step - loss: 0.8222 - val_loss: 0.8219\n",
            "Epoch 35/100\n",
            "142403/142403 [==============================] - 3s 18us/step - loss: 0.8220 - val_loss: 0.8218\n",
            "Epoch 36/100\n",
            "142403/142403 [==============================] - 3s 18us/step - loss: 0.8219 - val_loss: 0.8218\n",
            "Epoch 37/100\n",
            "142403/142403 [==============================] - 3s 20us/step - loss: 0.8217 - val_loss: 0.8215\n",
            "Epoch 38/100\n",
            "142403/142403 [==============================] - 2s 18us/step - loss: 0.8216 - val_loss: 0.8215\n",
            "Epoch 39/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8215 - val_loss: 0.8213\n",
            "Epoch 40/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8214 - val_loss: 0.8213\n",
            "Epoch 41/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8214 - val_loss: 0.8211\n",
            "Epoch 42/100\n",
            "142403/142403 [==============================] - 3s 18us/step - loss: 0.8212 - val_loss: 0.8209\n",
            "Epoch 43/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8210 - val_loss: 0.8209\n",
            "Epoch 44/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8210 - val_loss: 0.8208\n",
            "Epoch 45/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8209 - val_loss: 0.8208\n",
            "Epoch 46/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8208 - val_loss: 0.8209\n",
            "Epoch 47/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8207 - val_loss: 0.8205\n",
            "Epoch 48/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8207 - val_loss: 0.8206\n",
            "Epoch 49/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8206 - val_loss: 0.8205\n",
            "Epoch 50/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8205 - val_loss: 0.8203\n",
            "Epoch 51/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8204 - val_loss: 0.8202\n",
            "Epoch 52/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8203 - val_loss: 0.8202\n",
            "Epoch 53/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8203 - val_loss: 0.8202\n",
            "Epoch 54/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8203 - val_loss: 0.8201\n",
            "Epoch 55/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8202 - val_loss: 0.8202\n",
            "Epoch 56/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8201 - val_loss: 0.8200\n",
            "Epoch 57/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8200 - val_loss: 0.8197\n",
            "Epoch 58/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8200 - val_loss: 0.8199\n",
            "Epoch 59/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8199 - val_loss: 0.8196\n",
            "Epoch 60/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8199 - val_loss: 0.8197\n",
            "Epoch 61/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8198 - val_loss: 0.8197\n",
            "Epoch 62/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8197 - val_loss: 0.8198\n",
            "Epoch 63/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8197 - val_loss: 0.8195\n",
            "Epoch 64/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8196 - val_loss: 0.8195\n",
            "Epoch 65/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8195 - val_loss: 0.8195\n",
            "Epoch 66/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8194 - val_loss: 0.8193\n",
            "Epoch 67/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8194 - val_loss: 0.8192\n",
            "Epoch 68/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8194 - val_loss: 0.8192\n",
            "Epoch 69/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8193 - val_loss: 0.8191\n",
            "Epoch 70/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8193 - val_loss: 0.8192\n",
            "Epoch 71/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8192 - val_loss: 0.8196\n",
            "Epoch 72/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8192 - val_loss: 0.8194\n",
            "Epoch 73/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8191 - val_loss: 0.8190\n",
            "Epoch 74/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8190 - val_loss: 0.8189\n",
            "Epoch 75/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8192 - val_loss: 0.8190\n",
            "Epoch 76/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8191 - val_loss: 0.8188\n",
            "Epoch 77/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8189 - val_loss: 0.8189\n",
            "Epoch 78/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8190 - val_loss: 0.8191\n",
            "Epoch 79/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8188 - val_loss: 0.8187\n",
            "Epoch 80/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8188 - val_loss: 0.8186\n",
            "Epoch 81/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8189 - val_loss: 0.8184\n",
            "Epoch 82/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8186 - val_loss: 0.8185\n",
            "Epoch 83/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8188 - val_loss: 0.8185\n",
            "Epoch 84/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8188 - val_loss: 0.8184\n",
            "Epoch 85/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8186 - val_loss: 0.8189\n",
            "Epoch 86/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8187 - val_loss: 0.8184\n",
            "Epoch 87/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8185 - val_loss: 0.8186\n",
            "Epoch 88/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8186 - val_loss: 0.8184\n",
            "Epoch 89/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8186 - val_loss: 0.8186\n",
            "Epoch 90/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8186 - val_loss: 0.8182\n",
            "Epoch 91/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8185 - val_loss: 0.8186\n",
            "Epoch 92/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8185 - val_loss: 0.8182\n",
            "Epoch 93/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8183 - val_loss: 0.8183\n",
            "Epoch 94/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8184 - val_loss: 0.8183\n",
            "Epoch 95/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8182 - val_loss: 0.8184\n",
            "Epoch 96/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8183 - val_loss: 0.8180\n",
            "Epoch 97/100\n",
            "142403/142403 [==============================] - 2s 17us/step - loss: 0.8183 - val_loss: 0.8182\n",
            "Epoch 98/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8182 - val_loss: 0.8182\n",
            "Epoch 99/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8182 - val_loss: 0.8182\n",
            "Epoch 100/100\n",
            "142403/142403 [==============================] - 2s 16us/step - loss: 0.8181 - val_loss: 0.8182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3432236dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MntASWT9EYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_all = Model(input_layer_all,encoded_all)\n",
        "enc_all = encoder_all.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYRDsZ6ygSJq",
        "colab_type": "text"
      },
      "source": [
        "Loading library for k-NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iix7bOJQ5gxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaDHQXxt5iCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn_model = KNeighborsClassifier(n_neighbors=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjqB805u9XqF",
        "colab_type": "code",
        "outputId": "28832207-2428-4477-973e-760c44169ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Train the model using the training sets\n",
        "knn_model.fit(enc_all,y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80H7xbeSBvWc",
        "colab_type": "code",
        "outputId": "ff0fe410-fda3-4ca8-ebbf-108b745579ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%%time\n",
        "knn_predicted= knn_model.predict(encoder_all.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 34.1 s, sys: 130 ms, total: 34.2 s\n",
            "Wall time: 33.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aVN3Qe-D3Oz",
        "colab_type": "code",
        "outputId": "33126592-8277-4c98-8167-4be8a5159c2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "conf_matrix = metrics.confusion_matrix(y_test,knn_predicted)\n",
        "\n",
        "ax=plt.subplot()\n",
        "sns.heatmap(conf_matrix,annot=True,ax=ax,fmt='g')#annot=True to annotate cells, fmt='g' numbers not scientific form\n",
        "ax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(['Normal', 'Fraud']); ax.yaxis.set_ticklabels(['Normal', 'Fraud']);\n",
        "ax.set(yticks=[0, 2], \n",
        "       xticks=[0.5, 1.5])\n",
        "ax.yaxis.set_major_locator(ticker.IndexLocator(base=1, offset=0.5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEWCAYAAACAOivfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6E/gPmEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}